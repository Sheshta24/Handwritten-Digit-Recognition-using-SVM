
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Sat Mar  9 11:51:51 2024

@author: sheshta
"""

import numpy as np
from sklearn import datasets
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt

# Load the digits dataset (which is a smaller subset of the MNIST dataset)
digits = datasets.load_digits()

# Preprocess the data: Scale data to have mean 0 and variance 1
scaler = StandardScaler()
scaled_X = scaler.fit_transform(digits.data)

# Split the data into 80% training and 20% testing
X_train, X_test, y_train, y_test = train_test_split(scaled_X, digits.target, test_size=0.2, random_state=42)

# Convert y to -1 and 1 labels for a binary classification task
y_train_binary = np.where(y_train >= 5, 1, -1)
y_test_binary = np.where(y_test >= 5, 1, -1)

# Define the SVM with RBF kernel
class SVM_RBF:
    def __init__(self, C=1.0, gamma=0.1, n_iters=1000):
        self.C = C
        self.gamma = gamma
        self.n_iters = n_iters
        self.alphas = None
        self.support_vectors = None
        self.support_vector_labels = None

    def rbf_kernel(self, X1, X2):
        sq_dist = np.sum(X1**2, axis=1).reshape(-1, 1) + np.sum(X2**2, axis=1) - 2 * np.dot(X1, X2.T)
        return np.exp(-self.gamma * sq_dist)

    def fit(self, X, y):
        n_samples, _ = X.shape
        self.alphas = np.zeros(n_samples)
        kernel = self.rbf_kernel(X, X)

        for _ in range(self.n_iters):
            for i in range(n_samples):
                gradient = 1 - y[i] * np.sum(self.alphas * y * kernel[i])
                if gradient > 0:
                    self.alphas[i] += 1.0 / (self.C * gradient)

        self.support_vectors = X[self.alphas > 1e-5]
        self.support_vector_labels = y[self.alphas > 1e-5]
        self.alphas = self.alphas[self.alphas > 1e-5]

    def predict(self, X):
        kernel = self.rbf_kernel(X, self.support_vectors)
        predictions = np.sign(np.dot(kernel, self.alphas * self.support_vector_labels))
        return predictions

# Initialize and train the SVM model
svm = SVM_RBF(C=1.0, gamma=0.1, n_iters=1000)
svm.fit(X_train, y_train_binary)

# Make predictions with the trained model
predictions = svm.predict(X_test)

# Function to calculate accuracy
def accuracy(y_true, y_pred):
    return np.sum(y_true == y_pred) / len(y_true)

# Calculate the accuracy of the model
acc = accuracy(y_test_binary, predictions)
print(f"SVM classification accuracy with RBF kernel: {acc:.3f}")

# Generate the confusion matrix
conf_matrix = confusion_matrix(y_test_binary, predictions)

# Plot the confusion matrix
plt.figure(figsize=(8, 8))
plt.imshow(conf_matrix, interpolation='nearest', cmap=plt.cm.Blues)
plt.title('Confusion Matrix')
plt.colorbar()
classes = ['-1', '1']
tick_marks = np.arange(len(classes))
plt.xticks(tick_marks, classes)
plt.yticks(tick_marks, classes)

for i in range(conf_matrix.shape[0]):
    for j in range(conf_matrix.shape[1]):
        plt.text(j, i, conf_matrix[i, j],
                 horizontalalignment="center",
                 color="white" if conf_matrix[i, j] > conf_matrix.max() / 2 else "black")

plt.tight_layout()
plt.ylabel('True label')
plt.xlabel('Predicted label')
plt.show()
